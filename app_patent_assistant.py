import streamlit as st
import os
import requests
import json
import re

# LangChain ë° Gemini ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- 1. ì• í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë³¸ ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ---
st.set_page_config(page_title="AI íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸", layout="wide")

# [í”„ë¡¬í”„íŠ¸ 1] ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
KEYWORD_EXTRACTION_PROMPT = PromptTemplate.from_template(
    """
    You are an expert in semiconductor and patent search. Your task is to extract the most relevant and effective search keywords from the user's question.
    The keywords should be concise technical terms. Output them as a comma-separated list.
    
    User's Question: {question}
    
    Keywords:
    """
)

# [í”„ë¡¬í”„íŠ¸ 2] ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸
FINAL_ANSWER_PROMPT = PromptTemplate.from_template(
    """
    You are a helpful AI assistant specializing in patent analysis.
    Based ONLY on the following retrieved patent documents, provide a comprehensive answer to the user's original question.
    If the documents don't provide enough information, clearly state that the answer cannot be found in the provided documents.
    Provide a clear and concise answer, and always cite the source patent documents you used by their filenames (e.g., `[us20230012345a1p.txt]`).

    **Retrieved Documents:**
    {context}

    **User's Original Question:**
    {question}

    **Your Comprehensive Answer:**
    """
)

# --- 2. ì‚¬ì´ë“œë°” - ì„¤ì • ---
with st.sidebar:
    st.header("âœ¨ AI & DB ì„œë²„ ì„¤ì •")
    gemini_api_key = st.text_input("Gemini API Key", type="password")
    db_server_url = st.text_input("DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œ", help="ì˜ˆ: http://localhost:8000")
    
    st.markdown("---")
    st.header("ğŸ¤– ëª¨ë¸ ì„ íƒ")
    # [ìˆ˜ì •] ì‚¬ìš©ìê°€ Gemini 2.5 ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ì˜µì…˜ ì¶”ê°€
    selected_model = st.radio(
        "ë‹µë³€ ìƒì„± ëª¨ë¸ ì„ íƒ:",
        ("gemini-2.5-pro", "gemini-2.5-flash"),
        captions=["ìµœê³  í’ˆì§ˆ (2.5 Pro)", "ìµœì‹ /ê· í˜• (2.5 Flash)"],
        horizontal=True,
        index=0 # ê¸°ë³¸ê°’ìœ¼ë¡œ 2.5 Pro ì„ íƒ
    )

    st.markdown("---")
    st.header("ğŸ“š ë¶„ì„ ëŒ€ìƒ ì„ íƒ")
    db_options = {"3D DRAM íŠ¹í—ˆ": "3d_dram"}
    selected_db_name = st.selectbox("ë¶„ì„í•  DBë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=db_options.keys())
    selected_db_id = db_options[selected_db_name]
    
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# --- 3. ë©”ì¸ Q&A ë¡œì§ (AI ì—ì´ì „íŠ¸ ë£¨í‹´) ---
st.title(f"ğŸ¤– AI íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸ ({selected_db_name})")

if not gemini_api_key or not db_server_url:
    st.info("ì‚¬ì´ë“œë°”ì— Gemini API Keyì™€ DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    if "messages" not in st.session_state or st.session_state.get("current_model") != selected_model:
        st.session_state.messages = []
        st.session_state.current_model = selected_model

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_question := st.chat_input("ë¶„ì„í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            try:
                # [ìˆ˜ì •] ì„ íƒëœ ëª¨ë¸ë¡œ LLM ê°ì²´ ìƒì„±
                llm = ChatGoogleGenerativeAI(model=selected_model, google_api_key=gemini_api_key, temperature=0.0)

                # --- ë£¨í‹´ 2 & 3: Geminiì—ê²Œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ ---
                with st.spinner("1/3: ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                    keyword_chain = KEYWORD_EXTRACTION_PROMPT | llm | StrOutputParser()
                    extracted_keywords = keyword_chain.invoke({"question": user_question})
                    keyword_list = [k.strip() for k in extracted_keywords.split(',') if k.strip()]
                    st.info(f"ğŸ” ì¶”ì¶œëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: `{', '.join(keyword_list)}`")

                # --- ë£¨í‹´ 4: í‚¤ì›Œë“œë¡œ DB ì„œë²„ì— ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­ ---
                with st.spinner(f"2/3: DB ì„œë²„ì—ì„œ '{len(keyword_list)}'ê°œ í‚¤ì›Œë“œë¡œ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                    search_url = f"{db_server_url.rstrip('/')}/search_by_keywords"
                    search_payload = {"db_id": selected_db_id, "keywords": keyword_list}
                    response = requests.post(search_url, json=search_payload, timeout=60)
                    response.raise_for_status()
                    retrieved_data = response.json().get('documents', [])
                
                if not retrieved_data:
                    st.warning("ê´€ë ¨ëœ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
                else:
                    st.success(f"ğŸ“„ ì´ {len(retrieved_data)}ê°œì˜ ê´€ë ¨ íŠ¹í—ˆë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ê° ë¬¸ì„œë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

                    # --- ë£¨í‹´ 5: ê²€ìƒ‰ëœ ë¬¸ì„œë¡œ Geminiì—ê²Œ ìµœì¢… ë‹µë³€ ìƒì„± ìš”ì²­ ---
                    with st.spinner("3/3: Geminiê°€ ê²€ìƒ‰ëœ ë¬¸í—Œì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì¤‘..."):
                        
                        def format_docs(docs):
                            return "\n\n".join([f"--- Source: {os.path.basename(doc['metadata'].get('source', 'N/A'))} ---\n{doc['page_content']}" for doc in docs])
                        
                        final_rag_chain = (
                            {"context": lambda x: format_docs(retrieved_data), "question": RunnablePassthrough()}
                            | FINAL_ANSWER_PROMPT
                            | llm
                            | StrOutputParser()
                        )
                        
                        final_answer = final_rag_chain.invoke(user_question)
                        st.markdown(final_answer)
                        st.session_state.messages.append({"role": "assistant", "content": final_answer})

            except Exception as e:
                st.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
