import streamlit as st
import os
import requests
import json
import re

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- 1. ì• í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë³¸ ì„¤ì • ë° í”„ë¡¬í”„íŠ¸ ---
st.set_page_config(page_title="AI íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸", layout="wide")

# [í”„ë¡¬í”„íŠ¸ 1] í‚¤ì›Œë“œ ì¶”ì¶œìš©
KEYWORD_EXTRACTION_PROMPT = PromptTemplate.from_template(
    """
    You are an expert in semiconductor and patent search. Your task is to extract the most relevant and effective search keywords from the user's question.
    The keywords should be concise technical terms. Output them as a comma-separated list.
    
    User's Question: {question}
    
    Keywords:
    """
)

# [í”„ë¡¬í”„íŠ¸ 2] ì¢…í•© ë‹µë³€ ìƒì„±ìš©
FINAL_ANSWER_PROMPT = PromptTemplate.from_template(
    """
    You are a helpful AI assistant specializing in patent analysis.
    Based ONLY on the following retrieved patent documents, provide a comprehensive answer to the user's original question.
    If the documents don't provide enough information, clearly state that the answer cannot be found in the provided documents.
    Provide a clear and concise answer, and always cite the source patent documents you used by their filenames (e.g., `[us20230012345a1p.txt]`).

    **Retrieved Documents:**
    {context}

    **User's Original Question:**
    {question}

    **Your Comprehensive Answer:**
    """
)

# [í”„ë¡¬í”„íŠ¸ 3] ë‹¨ì¼ íŒŒì¼ ìš”ì•½ìš©
SINGLE_DOC_SUMMARY_PROMPT = PromptTemplate.from_template(
    """
    You are a patent analysis assistant. Provide a detailed summary of the following patent document's text.
    Explain the core technology, its purpose, and key structural or process features.
    
    **Patent Text:**
    {context}
    
    **Your Detailed Summary:**
    """
)

# --- 2. ì‚¬ì´ë“œë°” - ì„¤ì • ---
with st.sidebar:
    st.header("âœ¨ AI & DB ì„œë²„ ì„¤ì •")
    gemini_api_key = st.text_input("Gemini API Key", type="password")
    db_server_url = st.text_input("DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œ", help="ì˜ˆ: http://localhost:8000")
    
    st.markdown("---")
    st.header("ğŸ¤– ëª¨ë¸ ì„ íƒ")
    selected_model = st.radio(
        "ë‹µë³€ ìƒì„± ëª¨ë¸ ì„ íƒ:",
        ("gemini-2.5-pro", "gemini-2.5-flash"),
        captions=["ìµœê³  í’ˆì§ˆ (2.5 Pro)", "ìµœì‹ /ê· í˜• (2.5 Flash)"],
        horizontal=True
    )

    st.markdown("---")
    st.header("ğŸ“š ë¶„ì„ ëŒ€ìƒ ì„ íƒ")
    db_options = {"3D DRAM íŠ¹í—ˆ": "3d_dram"}
    selected_db_name = st.selectbox("ë¶„ì„í•  DBë¥¼ ì„ íƒí•˜ì„¸ìš”.", options=db_options.keys())
    selected_db_id = db_options[selected_db_name]
    
    if st.button("ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        st.session_state.messages = []
        st.rerun()

# [ì—…ê·¸ë ˆì´ë“œ] íŠ¹í—ˆ ë²ˆí˜¸ë¥¼ ë” ìœ ì—°í•˜ê²Œ ê°ì§€í•˜ëŠ” ì •ê·œ í‘œí˜„ì‹
# (US|KR|CN|JP|EP)ë¡œ ì‹œì‘í•˜ê³ , ì¤‘ê°„ì— ê³µë°±, ì , í•˜ì´í”ˆì´ ìˆì–´ë„ ë˜ë©°, ë’¤ì— ë¬¸ì(A1, B, P ë“±)ê°€ ë¶™ì–´ë„ ë˜ëŠ” íŒ¨í„´
PATENT_NUMBER_REGEX = re.compile(r'((?:US|KR|CN|JP|EP)[\s.-]?\d{4,}[\s.-]?\d+[A-Z\d]*)', re.IGNORECASE)

# --- 3. ë©”ì¸ Q&A ë¡œì§ (ì§€ëŠ¥í˜• ë“€ì–¼ ëª¨ë“œ) ---
st.title(f"ğŸ¤– AI íŠ¹í—ˆ ë¶„ì„ ì—ì´ì „íŠ¸ ({selected_db_name})")

if not gemini_api_key or not db_server_url:
    st.info("ì‚¬ì´ë“œë°”ì— Gemini API Keyì™€ DB ê²€ìƒ‰ ì„œë²„ ì£¼ì†Œë¥¼ ëª¨ë‘ ì…ë ¥í•´ì£¼ì„¸ìš”.")
else:
    if "messages" not in st.session_state or st.session_state.get("current_model") != selected_model:
        st.session_state.messages = []
        st.session_state.current_model = selected_model

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_question := st.chat_input("ë¶„ì„í•˜ê³  ì‹¶ì€ ë‚´ìš©ì„ ììœ ë¡­ê²Œ ì§ˆë¬¸í•´ë³´ì„¸ìš”..."):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        with st.chat_message("assistant"):
            try:
                llm = ChatGoogleGenerativeAI(model=selected_model, google_api_key=gemini_api_key, temperature=0.1)
                
                # [í•µì‹¬] ì‚¬ìš©ìì˜ ì§ˆë¬¸ ìœ í˜•ì„ ë¨¼ì € íŒë‹¨
                patent_match = PATENT_NUMBER_REGEX.search(user_question)
                
                # --- ëª¨ë“œ 1: íŠ¹ì • íŠ¹í—ˆ ë²ˆí˜¸ ìš”ì•½ ---
                if patent_match:
                    patent_number = patent_match.group(1)
                    st.info(f"íŠ¹ì • íŠ¹í—ˆ '{patent_number}'ì— ëŒ€í•œ ìš”ì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤...")
                    
                    with st.spinner("DB ì„œë²„ì—ì„œ í•´ë‹¹ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                        search_url = f"{db_server_url.rstrip('/')}/search_by_keywords"
                        # í‚¤ì›Œë“œë¡œ íŠ¹í—ˆ ë²ˆí˜¸ë¥¼ ë³´ë‚´ë©´, ê°€ì¥ ìœ ì‚¬í•œ ìê¸° ìì‹ ì´ ê²€ìƒ‰ë¨
                        search_payload = {"db_id": selected_db_id, "keywords": [patent_number], "k_per_keyword": 1}
                        response = requests.post(search_url, json=search_payload, timeout=60)
                        response.raise_for_status()
                        retrieved_data = response.json().get('documents', [])

                    if not retrieved_data:
                        st.error(f"DBì—ì„œ '{patent_number}'ì— í•´ë‹¹í•˜ëŠ” íŠ¹í—ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        with st.spinner("Geminiê°€ í•´ë‹¹ íŠ¹í—ˆë¥¼ ì •ë°€ ìš”ì•½í•˜ëŠ” ì¤‘..."):
                            doc_content = retrieved_data[0]['page_content']
                            summary_chain = SINGLE_DOC_SUMMARY_PROMPT | llm | StrOutputParser()
                            final_answer = summary_chain.invoke({"context": doc_content})
                            st.markdown(final_answer)
                            st.session_state.messages.append({"role": "assistant", "content": final_answer})

                # --- ëª¨ë“œ 2: AI ë¦¬ì„œì¹˜ ì—ì´ì „íŠ¸ ---
                else:
                    with st.spinner("1/3: ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
                        keyword_chain = KEYWORD_EXTRACTION_PROMPT | llm | StrOutputParser()
                        extracted_keywords = keyword_chain.invoke({"question": user_question})
                        keyword_list = [k.strip() for k in extracted_keywords.split(',') if k.strip()]
                        st.info(f"ğŸ” ì¶”ì¶œëœ ê²€ìƒ‰ í‚¤ì›Œë“œ: `{', '.join(keyword_list)}`")

                    with st.spinner(f"2/3: DB ì„œë²„ì—ì„œ '{len(keyword_list)}'ê°œ í‚¤ì›Œë“œë¡œ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                        search_url = f"{db_server_url.rstrip('/')}/search_by_keywords"
                        search_payload = {"db_id": selected_db_id, "keywords": keyword_list}
                        response = requests.post(search_url, json=search_payload, timeout=60)
                        response.raise_for_status()
                        retrieved_data = response.json().get('documents', [])
                    
                    if not retrieved_data:
                        st.warning("ê´€ë ¨ëœ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.success(f"ğŸ“„ ì´ {len(retrieved_data)}ê°œì˜ ê´€ë ¨ íŠ¹í—ˆë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤. ì´ì œ ì¢…í•©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                        with st.spinner("3/3: Geminiê°€ ê²€ìƒ‰ëœ ë¬¸í—Œì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•˜ëŠ” ì¤‘..."):
                            def format_docs(docs):
                                return "\n\n".join([f"--- Source: {os.path.basename(doc['metadata'].get('source', 'N/A'))} ---\n{doc['page_content']}" for doc in docs])
                            
                            final_rag_chain = (
                                {"context": lambda x: format_docs(retrieved_data), "question": RunnablePassthrough()}
                                | FINAL_ANSWER_PROMPT
                                | llm
                                | StrOutputParser()
                            )
                            final_answer = final_rag_chain.invoke(user_question)
                            st.markdown(final_answer)
                            st.session_state.messages.append({"role": "assistant", "content": final_answer})

            except Exception as e:
                st.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
